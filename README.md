# ğŸ§  Zunneko LLM Intelligent Router Platform

A **production-ready multi-LLM routing platform** that intelligently selects the best LLM provider based on request complexity, caching, cost efficiency, and system health.

This system includes **streaming responses, observability, resilience patterns, distributed tracing, and cost tracking** â€” simulating real enterprise AI infrastructure.

---

# ğŸš€ Features

## ğŸ¤– Intelligent LLM Routing
Automatically selects best provider based on:

- Request complexity
- Cost optimization
- Provider availability
- System health
- Performance metrics

Supported Providers:

- Groq
- Gemini
- Mock fallback provider

---

## âš¡ Multi-Layer Caching

### Exact Cache
- Instant response for identical prompts

### Semantic Cache
- Uses embeddings similarity search
- Avoids duplicate LLM calls

---

## ğŸ“¡ Streaming Responses
Supports real-time token streaming via FastAPI.

---

## ğŸ“Š Observability & Monitoring

### Metrics Tracking
Tracks:

- Cache hit ratio
- Provider usage
- Token usage
- Cost estimation
- Latency metrics

### Distributed Tracing
Tracks:

- Cache lookup time
- Request analysis time
- Provider execution latency
- End-to-end request latency

---

## ğŸ›¡ Resilience Patterns

### Circuit Breaker
Automatically disables failing providers.

### Retry Logic
Retries failed provider calls with exponential backoff.

### Fallback Provider
Ensures system reliability.

---

## ğŸ’° Cost Tracking
Estimates cost based on token usage per provider.

---

## ğŸ“¦ Metrics Snapshot Export
Download JSON snapshot of system metrics.

---

# ğŸ— Architecture

User â†’ FastAPI â†’ Intelligent Router
â”‚
â”œâ”€â”€ Exact Cache
â”œâ”€â”€ Semantic Cache
â”œâ”€â”€ Request Analyzer
â”œâ”€â”€ Provider Selector
â”œâ”€â”€ Circuit Breaker
â”œâ”€â”€ Retry Handler
â”œâ”€â”€ LLM Provider
â”œâ”€â”€ Metrics Collector
â”œâ”€â”€ Distributed Tracing
â””â”€â”€ Cost Tracker

# ğŸ“ Project Structure

src/
â”‚
â”œâ”€â”€ cache/
â”œâ”€â”€ config/
â”œâ”€â”€ metrics/
â”œâ”€â”€ observability/
â”œâ”€â”€ providers/
â”œâ”€â”€ queue/
â”œâ”€â”€ resilience/
â”œâ”€â”€ router/
â”œâ”€â”€ settings.py
â”‚
â”œâ”€â”€ main.py â†’ FastAPI Server
â”œâ”€â”€ worker.py â†’ Background worker
â”œâ”€â”€ chat_ui.py â†’ Streaming UI


---

# âš™ï¸ Installation

## 1ï¸âƒ£ Clone Repository

```bash
git clone https://github.com/siddique0786/zunneko_llm_router.git
cd zunneko_llm_router

---

##2ï¸âƒ£ Create Virtual Environment
Windows
```bash
python -m venv venv
venv\Scripts\activate

**Linux / Mac**
```bash
python3 -m venv venv
source venv/bin/activate


## 3ï¸âƒ£ Install Dependencies
```bash
pip install -r requirements.txt

##4ï¸âƒ£ Setup Environment Variables
Create .env file:
```bash
GROQ_API_KEY=your_groq_key
GEMINI_API_KEY=your_gemini_key
REDIS_URL=redis://localhost:6379


ğŸ§° Required Services
Redis
Windows

Use Docker or Redis Windows build.

Linux
```bash
sudo apt install redis-server


Mac
```bash
brew install redis

Run Redis:
```bash
redis-server


###------
## â–¶ Running The Project
**Start Worker**:
```bash
python worker.py


**Start FastAPI Server**:
```bash
uvicorn src.main:app --reload

**Start Chat UI**:
```bash
python chat_ui.py


##ğŸŒ API Endpoints
**Streaming Endpoint**:
```bash
GET /stream?prompt=YourQuestion

**Metrics Download**:
```bash
GET /metrics/download


##ğŸ§ª Example Usage

**Open in browser:**
```bash
http://localhost:8000/stream?prompt=Explain AI simply

## ğŸ“Š Example Metrics Output
```bash
{
  "total_tokens": 500,
  "total_cost": 0.0021,
  "provider_usage": {
    "GroqProvider": 4
  }
}

## ğŸ”„ Routing Logic

**Provider selection depends on:**

    1 Prompt complexity

    2 Cache availability

    3 Circuit breaker state

    4 Retry success/failure